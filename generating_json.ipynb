{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e64372",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Raw Sign Images to Import-Ready Clustering JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a4a78f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 0: Setup and Imports\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from PIL import Image\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a832298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reorganized dataset into sign-first structure at: /home/vim/Desktop/asim_niaz/cuneiform-ocr-data/cropped_signs_metadata/generating_json/new_structure_data\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 1 — Reorganize Dataset: Period → Sign\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "src_root = \"/home/vim/Desktop/asim_niaz/cuneiform-ocr-data/cropped_signs_metadata/data_full\"               # current structure: period/sign/images\n",
    "dst_root = \"/home/vim/Desktop/asim_niaz/cuneiform-ocr-data/cropped_signs_metadata/generating_json/new_structure_data\"    # new structure: sign/period/images\n",
    "\n",
    "os.makedirs(dst_root, exist_ok=True)\n",
    "\n",
    "# Loop through periods and signs\n",
    "for period in sorted(os.listdir(src_root)):\n",
    "    period_path = os.path.join(src_root, period)\n",
    "    if not os.path.isdir(period_path):\n",
    "        continue\n",
    "\n",
    "    for sign in sorted(os.listdir(period_path)):\n",
    "        sign_path = os.path.join(period_path, sign)\n",
    "        if not os.path.isdir(sign_path):\n",
    "            continue\n",
    "\n",
    "        # destination folder: dataset_sign_model/sign/period/\n",
    "        dst_dir = os.path.join(dst_root, sign, period)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "        # link or copy images\n",
    "        for file in os.listdir(sign_path):\n",
    "            if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue\n",
    "\n",
    "            src_file = os.path.join(sign_path, file)\n",
    "            dst_file = os.path.join(dst_dir, file)\n",
    "\n",
    "            # create symlink (fast and storage-friendly)\n",
    "            if not os.path.exists(dst_file):\n",
    "                try:\n",
    "                    os.symlink(os.path.abspath(src_file), dst_file)\n",
    "                except OSError:\n",
    "                    # fallback: copy file if symlink not supported (e.g. Windows)\n",
    "                    shutil.copy2(src_file, dst_file)\n",
    "\n",
    "print(\"Reorganized dataset into sign-first structure at:\", dst_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c501f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 979 sign classes\n",
      " Fine-tuning Sign Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]: 100%|███████████████████████████████████████████████████████████| 2977/2977 [03:53<00:00, 12.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 3.1508 Acc: 37.20% | Val Loss: 2.1106 Acc: 54.60%\n",
      " Improved — saved to generating_json/resnet18_sign_model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 [Train]: 100%|███████████████████████████████████████████████████████████| 2977/2977 [03:33<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Train Loss: 1.7495 Acc: 61.17% | Val Loss: 1.6508 Acc: 62.75%\n",
      " Improved — saved to generating_json/resnet18_sign_model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 [Train]: 100%|███████████████████████████████████████████████████████████| 2977/2977 [03:33<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Train Loss: 1.1718 Acc: 73.36% | Val Loss: 1.5220 Acc: 65.37%\n",
      " Improved — saved to generating_json/resnet18_sign_model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 [Train]: 100%|███████████████████████████████████████████████████████████| 2977/2977 [03:33<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Train Loss: 0.7487 Acc: 83.17% | Val Loss: 1.5111 Acc: 65.68%\n",
      " Improved — saved to generating_json/resnet18_sign_model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 [Train]: 100%|███████████████████████████████████████████████████████████| 2977/2977 [03:33<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Train Loss: 0.4411 Acc: 90.69% | Val Loss: 1.5818 Acc: 64.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 [Train]: 100%|███████████████████████████████████████████████████████████| 2977/2977 [03:33<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Train Loss: 0.2602 Acc: 94.82% | Val Loss: 1.6689 Acc: 64.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 [Train]: 100%|███████████████████████████████████████████████████████████| 2977/2977 [03:33<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Train Loss: 0.1644 Acc: 96.81% | Val Loss: 1.7470 Acc: 63.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 [Train]: 100%|███████████████████████████████████████████████████████████| 2977/2977 [03:33<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Train Loss: 0.1099 Acc: 97.95% | Val Loss: 1.8165 Acc: 63.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 [Train]: 100%|███████████████████████████████████████████████████████████| 2977/2977 [03:33<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Train Loss: 0.0807 Acc: 98.53% | Val Loss: 1.8891 Acc: 64.00%\n",
      " Early stopping at epoch 9\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 2 — Fine-Tune ResNet18 on Sign Classes\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIG ---\n",
    "data_dir = dst_root #\"dataset_sign_model\"  # NEW root organized by sign\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "lr = 1e-4\n",
    "patience = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- TRANSFORMS ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# --- DATASET WITH PATHS ---\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        original_tuple = super().__getitem__(index)\n",
    "        path = self.imgs[index][0]\n",
    "        return original_tuple + (path,)\n",
    "\n",
    "dataset = ImageFolderWithPaths(root=data_dir, transform=transform)\n",
    "num_classes = len(dataset.classes)\n",
    "print(f\" Found {num_classes} sign classes\")\n",
    "\n",
    "# --- Split dataset ---\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- Model ---\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Freeze most layers except top\n",
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith(\"layer4\") and not name.startswith(\"fc\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "# --- Early stopping setup ---\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "best_model_path = \"generating_json/resnet18_sign_model_best.pth\"\n",
    "\n",
    "print(\" Fine-tuning Sign Model...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for imgs, labels, paths in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, paths in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\" Improved — saved to {best_model_path}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\" Early stopping at epoch {epoch+1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b157f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Sign Features: 100%|█████████████████████████████████████████████████████| 3721/3721 [04:05<00:00, 15.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 'sign_model_embeddings.npz'\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 3 — Extract Sign Embeddings\n",
    "# ============================================\n",
    "\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load best weights\n",
    "model = models.resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.load_state_dict(torch.load(\"generating_json/resnet18_sign_model_best.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Feature extractor\n",
    "feature_extractor = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Data loader (no split, full set)\n",
    "full_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "features, sign_labels, periods, paths = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls, img_paths in tqdm(full_loader, desc=\"Extracting Sign Features\"):\n",
    "        imgs = imgs.to(device)\n",
    "        out = feature_extractor(imgs).squeeze(-1).squeeze(-1)\n",
    "        features.append(out.cpu().numpy())\n",
    "        sign_labels.extend([dataset.classes[l] for l in lbls])\n",
    "        paths.extend(img_paths)\n",
    "        # extract period from folder structure\n",
    "        periods.extend([os.path.basename(os.path.dirname(p)) for p in img_paths])\n",
    "\n",
    "features = np.concatenate(features, axis=0)\n",
    "np.savez(\"generating_json/sign_model_embeddings.npz\",\n",
    "         features=features,\n",
    "         signs=np.array(sign_labels),\n",
    "         periods=np.array(periods),\n",
    "         paths=np.array(paths))\n",
    "print(\" Saved 'sign_model_embeddings.npz'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c15ab020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 119047 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sign-period groups: 100%|████████████████████████████████████████████████| 4513/4513 [03:02<00:00, 24.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 119047 path documents to generating_json/sign_clustering_paths.json\n",
      "Saved 119047 base64 documents to generating_json/sign_clustering_base64.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Sign Form Clustering and Dual JSON Export (paths + base64) from Embeddings\n",
    "# Field order matches CroppedSignImageSchema\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import uuid\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "\n",
    "EMBEDDINGS_PATH = \"generating_json/sign_model_embeddings.npz\"\n",
    "\n",
    "OUTPUT_PATH_BASE64 = \"generating_json/sign_clustering_base64.json\"\n",
    "OUTPUT_PATH_PATHS = \"generating_json/sign_clustering_paths.json\"\n",
    "\n",
    "MAX_CANONICAL_PER_PERIOD = 2\n",
    "MIN_CLUSTER_SIZE_FOR_VARIANT = 10\n",
    "MAX_K = 6\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "\n",
    "def image_to_base64(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def extract_fragment_number(image_path: str) -> str:\n",
    "    # Example: NCBT.280_sign29_11_Neo-Babylonian.jpg → NCBT.280\n",
    "    return os.path.basename(image_path).split(\"_\")[0]\n",
    "\n",
    "\n",
    "def choose_k(n: int) -> int:\n",
    "    if n < 20:\n",
    "        return 1\n",
    "    if n < 50:\n",
    "        return 2\n",
    "    if n < 100:\n",
    "        return 3\n",
    "    return min(MAX_K, n // 30)\n",
    "\n",
    "# =========================\n",
    "# Load embeddings\n",
    "# =========================\n",
    "\n",
    "data = np.load(EMBEDDINGS_PATH, allow_pickle=True)\n",
    "\n",
    "features = data[\"features\"]   # (N, D)\n",
    "signs = data[\"signs\"]\n",
    "periods = data[\"periods\"]\n",
    "paths = data[\"paths\"]\n",
    "\n",
    "print(f\"Loaded {len(features)} embeddings\")\n",
    "\n",
    "# =========================\n",
    "# Group by (sign, period)\n",
    "# =========================\n",
    "\n",
    "group_index = defaultdict(list)\n",
    "for i, (s, p) in enumerate(zip(signs, periods)):\n",
    "    group_index[(s, p)].append(i)\n",
    "\n",
    "docs_base64 = []\n",
    "docs_paths = []\n",
    "\n",
    "# =========================\n",
    "# Clustering + JSON creation\n",
    "# =========================\n",
    "\n",
    "for (sign, period), idxs in tqdm(group_index.items(), desc=\"Processing sign-period groups\"):\n",
    "    feats = features[idxs]\n",
    "    img_paths = paths[idxs]\n",
    "\n",
    "    n = len(feats)\n",
    "    k = choose_k(n)\n",
    "\n",
    "    if k == 1:\n",
    "        labels = np.zeros(n, dtype=int)\n",
    "    else:\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=k,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_init=10\n",
    "        )\n",
    "        labels = kmeans.fit_predict(feats)\n",
    "\n",
    "    # group by cluster\n",
    "    clusters = defaultdict(list)\n",
    "    for i, lbl in enumerate(labels):\n",
    "        clusters[lbl].append(i)\n",
    "\n",
    "    # sort clusters by size (largest first)\n",
    "    sorted_clusters = sorted(\n",
    "        clusters.items(),\n",
    "        key=lambda x: len(x[1]),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    for rank, (_, members) in enumerate(sorted_clusters):\n",
    "        cluster_feats = feats[members]\n",
    "        cluster_size = len(members)\n",
    "\n",
    "        # centroid selection\n",
    "        centroid = cluster_feats.mean(axis=0)\n",
    "        dists = cosine_distances([centroid], cluster_feats)[0]\n",
    "        rep_local_idx = int(np.argmin(dists))\n",
    "        rep_idx = members[rep_local_idx]\n",
    "\n",
    "        # -------- form labeling --------\n",
    "        if rank < MAX_CANONICAL_PER_PERIOD:\n",
    "            form_label = f\"canonical{rank + 1}\"\n",
    "        else:\n",
    "            form_label = f\"variant{rank + 1 - MAX_CANONICAL_PER_PERIOD}\"\n",
    "\n",
    "        is_main = (\n",
    "            form_label.startswith(\"canonical\") or\n",
    "            cluster_size >= MIN_CLUSTER_SIZE_FOR_VARIANT\n",
    "        )\n",
    "\n",
    "        # -------- emit records --------\n",
    "        for m in members:\n",
    "            image_path = img_paths[m]\n",
    "\n",
    "            if not Path(image_path).exists():\n",
    "                continue\n",
    "\n",
    "            uid = str(uuid.uuid4())\n",
    "            fragment_number = extract_fragment_number(image_path)\n",
    "\n",
    "            # ----- path-based JSON (schema order) -----\n",
    "            docs_paths.append({\n",
    "                \"_id\": uid,\n",
    "                \"image_path\": image_path,\n",
    "                \"fragment_number\": fragment_number,\n",
    "\n",
    "                \"sign\": sign,\n",
    "                \"period\": period,\n",
    "                \"form\": form_label,\n",
    "\n",
    "                \"isCentroid\": (m == rep_idx),\n",
    "                \"clusterSize\": cluster_size,\n",
    "                \"isMain\": is_main,\n",
    "            })\n",
    "\n",
    "            # ----- base64-based JSON (schema order) -----\n",
    "            docs_base64.append({\n",
    "                \"_id\": uid,\n",
    "                \"image\": image_to_base64(image_path),\n",
    "                \"fragment_number\": fragment_number,\n",
    "\n",
    "                \"sign\": sign,\n",
    "                \"period\": period,\n",
    "                \"form\": form_label,\n",
    "\n",
    "                \"isCentroid\": (m == rep_idx),\n",
    "                \"clusterSize\": cluster_size,\n",
    "                \"isMain\": is_main,\n",
    "            })\n",
    "\n",
    "# =========================\n",
    "# Save outputs\n",
    "# =========================\n",
    "\n",
    "if docs_paths:\n",
    "    with open(OUTPUT_PATH_PATHS, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docs_paths, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(docs_paths)} path documents to {OUTPUT_PATH_PATHS}\")\n",
    "\n",
    "if docs_base64:\n",
    "    with open(OUTPUT_PATH_BASE64, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docs_base64, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(docs_base64)} base64 documents to {OUTPUT_PATH_BASE64}\")\n",
    "\n",
    "if not docs_paths and not docs_base64:\n",
    "    print(\"No documents generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3bc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
